@misc{Kornilov2025,
  author    = {Kornilov Nikita and Zmushko Philip and Semenov Andrei and Gasnikov Alexander and Beznosikov Alexander},
  title     = {Sign Operator for Coping with Heavy-Tailed Noise: High Probability Convergence Bounds with Extensions to Distributed Optimization and Comparison Oracle},
  year      = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2502.07923},
  groups    = {Key articles},
  keywords  = {Optimization and Control (math.OC), Machine Learning (cs.LG), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}
@InProceedings{pmlr-v80-bernstein18a,
  title = 	 {sign{SGD}: Compressed Optimisation for Non-Convex Problems},
  author =       {Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {560--569},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/bernstein18a/bernstein18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/bernstein18a.html},
  abstract = 	 {Training large neural networks requires distributing learning across multiple workers, where the cost of communicating gradients can be a significant bottleneck. signSGD alleviates this problem by transmitting just the sign of each minibatch stochastic gradient. We prove that it can get the best of both worlds: compressed gradients and SGD-level convergence rate. The relative $\ell_1/\ell_2$ geometry of gradients, noise and curvature informs whether signSGD or SGD is theoretically better suited to a particular problem. On the practical side we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models. We extend our theory to the distributed setting, where the parameter server uses majority vote to aggregate gradient signs from each worker enabling 1-bit compression of worker-server communication in both directions. Using a theorem by Gauss we prove that majority vote can achieve the same reduction in variance as full precision distributed SGD. Thus, there is great promise for sign-based optimisation schemes to achieve fast communication and fast convergence. Code to reproduce experiments is to be found at https://github.com/jxbz/signSGD.}
}
@misc{gorbunov,
      title={Methods for Convex $(L_0,L_1)$-Smooth Optimization: Clipping, Acceleration, and Adaptivity}, 
      author={Eduard Gorbunov and Nazarii Tupitsa and Sayantan Choudhury and Alen Aliev and Peter Richtárik and Samuel Horváth and Martin Takáč},
      year={2024},
      eprint={2409.14989},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2409.14989}, 
}
@inproceedings{NEURIPS2022_40924475,
 author = {Crawshaw, Michael and Liu, Mingrui and Orabona, Francesco and Zhang, Wei and Zhuang, Zhenxun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {9955--9968},
 publisher = {Curran Associates, Inc.},
 title = {Robustness to Unbounded Smoothness of Generalized SignSGD},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/40924475a9bf768bdac3725e67745283-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@InProceedings{karimireddy,
  title = 	 {Error Feedback Fixes {S}ign{SGD} and other Gradient Compression Schemes},
  author =       {Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3252--3261},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/karimireddy19a/karimireddy19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/karimireddy19a.html},
  abstract = 	 {Sign-based algorithms (e.g. signSGD) have been proposed as a biased gradient compression technique to alleviate the communication bottleneck in training large neural networks across multiple workers. We show simple convex counter-examples where signSGD does not converge to the optimum. Further, even when it does converge, signSGD may generalize poorly when compared with SGD. These issues arise because of the biased nature of the sign compression operator. We then show that using error-feedback, i.e. incorporating the error made by the compression operator into the next step, overcomes these issues. We prove that our algorithm (EF-SGD) with arbitrary compression operator achieves the same rate of convergence as SGD without any additional assumptions. Thus EF-SGD achieves gradient compression for free. Our experiments thoroughly substantiate the theory.}
}
@misc{hübler2024gradientclippingnormalizationheavy,
      title={From Gradient Clipping to Normalization for Heavy Tailed SGD}, 
      author={Florian Hübler and Ilyas Fatkhullin and Niao He},
      year={2024},
      eprint={2410.13849},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2410.13849}, 
}
@misc{zhang2020gradientclippingacceleratestraining,
      title={Why gradient clipping accelerates training: A theoretical justification for adaptivity}, 
      author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
      year={2020},
      eprint={1905.11881},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/1905.11881}, 
}