# Link Review

- 3 key articles for the research
- additional papers for specific methods and/or proofs

| Topic | Title | Year | Authors | Paper | Summary |
| :--- | :--- | ---: | :--- | :--- | :--- |
| Key article 1 | Sign Operator for Coping with Heavy-Tailed Noise | 2025 | Kornilov et al. | [arXiv](https://arxiv.org/abs/2502.07923) | Proofs for heavy-tailed noise |
| Key article 2 | signSGD: Compressed Optimisation for Non-Convex Problems | 2018 | J. Bernstein et al. | [PMLR](https://proceedings.mlr.press/v80/bernstein18a.html) | 3 Sign-based methods |
| Key article 3 | Methods for Convex (L0,L1)-Smooth Optimization: Clipping, Acceleration, and Adaptivity | 2024 | Gorbunov et al. | [arXiv](https://arxiv.org/abs/2409.14989) | New convergence guarantees for existing methods |
| Theory |Robustness to Unbounded Smoothness of Generalized SignSGD | 2022 | M. Crawshaw et al. | [Curran Associates](https://proceedings.neurips.cc/paper_files/paper/2022/file/40924475a9bf768bdac3725e67745283-Paper-Conference.pdf) | L_0, L1 Sign SGD with Momentum |
| Additional | Error Feedback Fixes SignSGD and other Gradient Compression Schemes | 2019 | Richt√°rik et al. | [PMLR]({https://proceedings.mlr.press/v97/karimireddy19a.html) | Check for convex case |
| From Gradient Clipping to Normalization for Heavy Tailed SGD | 2024 | Hubler et al. | [arXiv](https://arxiv.org/abs/2410.13849) | Heavy Tailed SGD |
| Why gradient clipping accelerates training: A theoretical justification for adaptivity | 2020 | Zhang et al. | [arXiv](https://arxiv.org/abs/1905.11881) |Intro to (L0,L1)-smoothness assump. |
